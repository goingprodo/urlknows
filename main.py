"""
URL Analytics Library - ì¢…í•©ì ì¸ URL ë¶„ì„ ë„êµ¬
pip install requests beautifulsoup4 selenium webdriver-manager matplotlib seaborn pandas nltk textstat whois python-whois
"""

import requests
from bs4 import BeautifulSoup
import time
import re
from urllib.parse import urljoin, urlparse, parse_qs
from collections import Counter, defaultdict
import json
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from datetime import datetime, timedelta
import whois
from textstat import flesch_reading_ease, flesch_kincaid_grade
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import ssl
import socket
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
import warnings
warnings.filterwarnings('ignore')

class URLAnalyzer:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ì²˜ìŒ ì‹¤í–‰ì‹œ)
        try:
            nltk.data.find('tokenizers/punkt')
            nltk.data.find('corpora/stopwords')
        except LookupError:
            nltk.download('punkt', quiet=True)
            nltk.download('stopwords', quiet=True)
    
    def analyze_url(self, url):
        """ë©”ì¸ ë¶„ì„ í•¨ìˆ˜ - ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜"""
        print(f"ğŸ” ë¶„ì„ ì‹œì‘: {url}")
        
        results = {
            'url': url,
            'timestamp': datetime.now().isoformat(),
            'basic_info': self.get_basic_info(url),
            'seo_analysis': self.analyze_seo(url),
            'performance': self.measure_performance(url),
            'content_analysis': self.analyze_content(url),
            'technical_analysis': self.analyze_technical(url),
            'security_analysis': self.analyze_security(url),
            'keyword_analysis': self.analyze_keywords(url),
            'social_media': self.analyze_social_media(url),
            'mobile_analysis': self.analyze_mobile_compatibility(url)
        }
        
        return results
    
    def get_basic_info(self, url):
        """ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘"""
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            return {
                'status_code': response.status_code,
                'response_time': response.elapsed.total_seconds(),
                'content_length': len(response.content),
                'content_type': response.headers.get('content-type', ''),
                'server': response.headers.get('server', ''),
                'title': soup.title.string if soup.title else '',
                'meta_description': self._get_meta_content(soup, 'description'),
                'language': soup.html.get('lang') if soup.html else '',
                'charset': self._extract_charset(response.headers.get('content-type', ''))
            }
        except Exception as e:
            return {'error': str(e)}
    
    def analyze_seo(self, url):
        """SEO ë¶„ì„"""
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë©”íƒ€ íƒœê·¸ ë¶„ì„
            meta_tags = {}
            for meta in soup.find_all('meta'):
                name = meta.get('name') or meta.get('property')
                content = meta.get('content')
                if name and content:
                    meta_tags[name] = content
            
            # í—¤ë”© íƒœê·¸ ë¶„ì„
            headings = {}
            for i in range(1, 7):
                h_tags = soup.find_all(f'h{i}')
                headings[f'h{i}'] = [tag.get_text().strip() for tag in h_tags]
            
            # ì´ë¯¸ì§€ ë¶„ì„
            images = soup.find_all('img')
            img_analysis = {
                'total_images': len(images),
                'images_without_alt': len([img for img in images if not img.get('alt')]),
                'images_without_title': len([img for img in images if not img.get('title')])
            }
            
            # ë§í¬ ë¶„ì„
            links = soup.find_all('a', href=True)
            internal_links = []
            external_links = []
            
            domain = urlparse(url).netloc
            for link in links:
                href = link['href']
                if href.startswith('http'):
                    if domain in href:
                        internal_links.append(href)
                    else:
                        external_links.append(href)
                elif href.startswith('/'):
                    internal_links.append(urljoin(url, href))
            
            return {
                'meta_tags': meta_tags,
                'headings': headings,
                'images': img_analysis,
                'links': {
                    'internal_count': len(internal_links),
                    'external_count': len(external_links),
                    'internal_links': internal_links[:10],  # ìƒìœ„ 10ê°œë§Œ
                    'external_links': external_links[:10]
                },
                'robots_txt': self._check_robots_txt(url),
                'sitemap': self._check_sitemap(url)
            }
        except Exception as e:
            return {'error': str(e)}
    
    def measure_performance(self, url):
        """ì„±ëŠ¥ ì¸¡ì •"""
        try:
            # ì—¬ëŸ¬ ë²ˆ ìš”ì²­í•˜ì—¬ í‰ê·  ì¸¡ì •
            times = []
            for _ in range(5):
                start_time = time.time()
                response = self.session.get(url, timeout=10)
                end_time = time.time()
                times.append(end_time - start_time)
                time.sleep(0.5)
            
            # DNS ì¡°íšŒ ì‹œê°„ ì¸¡ì •
            domain = urlparse(url).netloc
            dns_start = time.time()
            socket.gethostbyname(domain)
            dns_time = time.time() - dns_start
            
            return {
                'avg_response_time': sum(times) / len(times),
                'min_response_time': min(times),
                'max_response_time': max(times),
                'dns_lookup_time': dns_time,
                'content_size': len(response.content),
                'compression': 'gzip' in response.headers.get('content-encoding', ''),
                'caching': response.headers.get('cache-control', ''),
                'performance_score': self._calculate_performance_score(times, len(response.content))
            }
        except Exception as e:
            return {'error': str(e)}
    
    def analyze_content(self, url):
        """ì½˜í…ì¸  ë¶„ì„"""
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = soup.get_text()
            words = word_tokenize(text.lower())
            
            # ë¶ˆìš©ì–´ ì œê±°
            stop_words = set(stopwords.words('english')) | set(stopwords.words('korean') if 'korean' in stopwords.fileids() else [])
            filtered_words = [word for word in words if word.isalnum() and word not in stop_words]
            
            return {
                'word_count': len(filtered_words),
                'character_count': len(text),
                'paragraph_count': len(soup.find_all('p')),
                'reading_ease': flesch_reading_ease(text) if text else 0,
                'reading_grade': flesch_kincaid_grade(text) if text else 0,
                'most_common_words': dict(Counter(filtered_words).most_common(20)),
                'content_density': len(filtered_words) / len(response.content) * 1000 if response.content else 0
            }
        except Exception as e:
            return {'error': str(e)}
    
    def analyze_technical(self, url):
        """ê¸°ìˆ ì  ë¶„ì„"""
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # JavaScript ë° CSS íŒŒì¼ ë¶„ì„
            scripts = soup.find_all('script', src=True)
            stylesheets = soup.find_all('link', rel='stylesheet')
            
            # Schema.org ë§ˆí¬ì—… í™•ì¸
            schema_scripts = soup.find_all('script', type='application/ld+json')
            
            return {
                'doctype': str(soup.contents[0]) if soup.contents else '',
                'html5': '<!DOCTYPE html>' in response.text.upper(),
                'javascript_files': len(scripts),
                'css_files': len(stylesheets),
                'inline_scripts': len(soup.find_all('script', src=False)),
                'inline_styles': len(soup.find_all('style')),
                'schema_markup': len(schema_scripts),
                'viewport_meta': bool(soup.find('meta', attrs={'name': 'viewport'})),
                'responsive_design': self._check_responsive_design(soup),
                'technologies': self._detect_technologies(response.headers, soup)
            }
        except Exception as e:
            return {'error': str(e)}
    
    def analyze_security(self, url):
        """ë³´ì•ˆ ë¶„ì„"""
        try:
            response = self.session.get(url, timeout=10)
            
            # SSL ì¸ì¦ì„œ í™•ì¸
            is_https = url.startswith('https://')
            ssl_info = {}
            
            if is_https:
                domain = urlparse(url).netloc
                context = ssl.create_default_context()
                with socket.create_connection((domain, 443), timeout=10) as sock:
                    with context.wrap_socket(sock, server_hostname=domain) as ssock:
                        cert = ssock.getpeercert()
                        ssl_info = {
                            'issuer': dict(x[0] for x in cert['issuer']),
                            'subject': dict(x[0] for x in cert['subject']),
                            'expires': cert['notAfter']
                        }
            
            return {
                'https': is_https,
                'ssl_certificate': ssl_info,
                'security_headers': {
                    'strict_transport_security': response.headers.get('strict-transport-security'),
                    'content_security_policy': response.headers.get('content-security-policy'),
                    'x_frame_options': response.headers.get('x-frame-options'),
                    'x_content_type_options': response.headers.get('x-content-type-options')
                },
                'mixed_content': self._check_mixed_content(response.text, is_https)
            }
        except Exception as e:
            return {'error': str(e)}
    
    def analyze_keywords(self, url):
        """í‚¤ì›Œë“œ ë¶„ì„"""
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° í‚¤ì›Œë“œ ë¶„ì„
            title = soup.title.string if soup.title else ''
            meta_desc = self._get_meta_content(soup, 'description')
            headings = ' '.join([h.get_text() for h in soup.find_all(['h1', 'h2', 'h3'])])
            body_text = soup.get_text()
            
            # í‚¤ì›Œë“œ ë°€ë„ ê³„ì‚°
            all_text = f"{title} {meta_desc} {headings} {body_text}".lower()
            words = re.findall(r'\b\w+\b', all_text)
            word_freq = Counter(words)
            
            # ë¶ˆìš©ì–´ ì œê±°
            stop_words = set(stopwords.words('english'))
            filtered_freq = {word: freq for word, freq in word_freq.items() 
                           if word not in stop_words and len(word) > 2}
            
            total_words = len(words)
            keyword_density = {word: (freq/total_words)*100 
                             for word, freq in list(filtered_freq.items())[:20]}
            
            return {
                'total_words': total_words,
                'unique_words': len(set(words)),
                'keyword_density': keyword_density,
                'top_keywords': dict(Counter(filtered_freq).most_common(20)),
                'title_keywords': Counter(re.findall(r'\b\w+\b', title.lower())),
                'meta_keywords': self._get_meta_content(soup, 'keywords')
            }
        except Exception as e:
            return {'error': str(e)}
    
    def analyze_social_media(self, url):
        """ì†Œì…œ ë¯¸ë””ì–´ ë¶„ì„"""
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Open Graph íƒœê·¸
            og_tags = {}
            for meta in soup.find_all('meta', property=re.compile(r'^og:')):
                og_tags[meta['property']] = meta.get('content', '')
            
            # Twitter Card íƒœê·¸
            twitter_tags = {}
            for meta in soup.find_all('meta', attrs={'name': re.compile(r'^twitter:')}):
                twitter_tags[meta['name']] = meta.get('content', '')
            
            # ì†Œì…œ ë¯¸ë””ì–´ ë§í¬ ì°¾ê¸°
            social_links = []
            social_domains = ['facebook.com', 'twitter.com', 'instagram.com', 'linkedin.com', 
                            'youtube.com', 'tiktok.com', 'pinterest.com']
            
            for link in soup.find_all('a', href=True):
                href = link['href']
                for domain in social_domains:
                    if domain in href:
                        social_links.append({'platform': domain, 'url': href})
            
            return {
                'open_graph': og_tags,
                'twitter_cards': twitter_tags,
                'social_links': social_links,
                'social_share_buttons': len(soup.find_all('a', href=re.compile(r'(facebook|twitter|linkedin)\.com/share')))
            }
        except Exception as e:
            return {'error': str(e)}
    
    def analyze_mobile_compatibility(self, url):
        """ëª¨ë°”ì¼ í˜¸í™˜ì„± ë¶„ì„"""
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë·°í¬íŠ¸ ë©”íƒ€ íƒœê·¸ í™•ì¸
            viewport = soup.find('meta', attrs={'name': 'viewport'})
            viewport_content = viewport.get('content', '') if viewport else ''
            
            # ë°˜ì‘í˜• ë””ìì¸ ìš”ì†Œ í™•ì¸
            media_queries = len(re.findall(r'@media', response.text))
            
            return {
                'viewport_meta': bool(viewport),
                'viewport_content': viewport_content,
                'media_queries_count': media_queries,
                'responsive_images': len(soup.find_all('img', srcset=True)),
                'mobile_friendly_score': self._calculate_mobile_score(viewport_content, media_queries)
            }
        except Exception as e:
            return {'error': str(e)}
    
    def generate_report(self, analysis_results, save_path=None):
        """ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = f"""
# URL ë¶„ì„ ë¦¬í¬íŠ¸
ë¶„ì„ URL: {analysis_results['url']}
ë¶„ì„ ì‹œê°„: {analysis_results['timestamp']}

## ğŸ“Š ê¸°ë³¸ ì •ë³´
- ìƒíƒœ ì½”ë“œ: {analysis_results['basic_info'].get('status_code', 'N/A')}
- ì‘ë‹µ ì‹œê°„: {analysis_results['basic_info'].get('response_time', 'N/A')}ì´ˆ
- ì½˜í…ì¸  í¬ê¸°: {analysis_results['basic_info'].get('content_length', 'N/A')} bytes
- í˜ì´ì§€ ì œëª©: {analysis_results['basic_info'].get('title', 'N/A')}

## ğŸ” SEO ë¶„ì„
- ë‚´ë¶€ ë§í¬: {analysis_results['seo_analysis'].get('links', {}).get('internal_count', 0)}ê°œ
- ì™¸ë¶€ ë§í¬: {analysis_results['seo_analysis'].get('links', {}).get('external_count', 0)}ê°œ
- ì´ ì´ë¯¸ì§€: {analysis_results['seo_analysis'].get('images', {}).get('total_images', 0)}ê°œ
- ALT íƒœê·¸ ì—†ëŠ” ì´ë¯¸ì§€: {analysis_results['seo_analysis'].get('images', {}).get('images_without_alt', 0)}ê°œ

## âš¡ ì„±ëŠ¥ ë¶„ì„
- í‰ê·  ì‘ë‹µ ì‹œê°„: {analysis_results['performance'].get('avg_response_time', 0):.2f}ì´ˆ
- DNS ì¡°íšŒ ì‹œê°„: {analysis_results['performance'].get('dns_lookup_time', 0):.2f}ì´ˆ
- ì„±ëŠ¥ ì ìˆ˜: {analysis_results['performance'].get('performance_score', 0):.1f}/100

## ğŸ“ ì½˜í…ì¸  ë¶„ì„
- ë‹¨ì–´ ìˆ˜: {analysis_results['content_analysis'].get('word_count', 0)}ê°œ
- ë¬¸ë‹¨ ìˆ˜: {analysis_results['content_analysis'].get('paragraph_count', 0)}ê°œ
- ê°€ë…ì„± ì ìˆ˜: {analysis_results['content_analysis'].get('reading_ease', 0):.1f}

## ğŸ”’ ë³´ì•ˆ ë¶„ì„
- HTTPS ì‚¬ìš©: {'âœ…' if analysis_results['security_analysis'].get('https') else 'âŒ'}
- ë³´ì•ˆ í—¤ë” ì„¤ì •: {len([h for h in analysis_results['security_analysis'].get('security_headers', {}).values() if h])}ê°œ

## ğŸ“± ëª¨ë°”ì¼ í˜¸í™˜ì„±
- ë·°í¬íŠ¸ ë©”íƒ€ íƒœê·¸: {'âœ…' if analysis_results['mobile_analysis'].get('viewport_meta') else 'âŒ'}
- ëª¨ë°”ì¼ ì¹œí™”ì„± ì ìˆ˜: {analysis_results['mobile_analysis'].get('mobile_friendly_score', 0):.1f}/100
        """
        
        if save_path:
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"ğŸ“„ ë¦¬í¬íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}")
        
        return report
    
    def create_dashboard_data(self, analysis_results):
        """ëŒ€ì‹œë³´ë“œìš© ë°ì´í„° ìƒì„±"""
        dashboard_data = {
            'stats': {
                'total_visitors': analysis_results['performance'].get('performance_score', 0) * 23,  # ì˜ˆì‹œ
                'page_views': analysis_results['content_analysis'].get('word_count', 0),
                'clicks': analysis_results['seo_analysis'].get('links', {}).get('internal_count', 0) * 10,
                'avg_session': analysis_results['performance'].get('avg_response_time', 0) * 100
            },
            'keywords': list(analysis_results['keyword_analysis'].get('top_keywords', {}).items())[:15],
            'performance_data': {
                'response_times': [analysis_results['performance'].get('min_response_time', 0),
                                 analysis_results['performance'].get('avg_response_time', 0),
                                 analysis_results['performance'].get('max_response_time', 0)],
                'scores': {
                    'seo': len(analysis_results['seo_analysis'].get('meta_tags', {})) * 10,
                    'performance': analysis_results['performance'].get('performance_score', 0),
                    'security': 85 if analysis_results['security_analysis'].get('https') else 45,
                    'mobile': analysis_results['mobile_analysis'].get('mobile_friendly_score', 0)
                }
            }
        }
        return dashboard_data
    
    # í—¬í¼ ë©”ì„œë“œë“¤
    def _get_meta_content(self, soup, name):
        meta = soup.find('meta', attrs={'name': name}) or soup.find('meta', attrs={'property': name})
        return meta.get('content', '') if meta else ''
    
    def _extract_charset(self, content_type):
        if 'charset=' in content_type:
            return content_type.split('charset=')[1].split(';')[0]
        return ''
    
    def _check_robots_txt(self, url):
        try:
            robots_url = urljoin(url, '/robots.txt')
            response = self.session.get(robots_url, timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def _check_sitemap(self, url):
        try:
            sitemap_url = urljoin(url, '/sitemap.xml')
            response = self.session.get(sitemap_url, timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def _calculate_performance_score(self, times, content_size):
        avg_time = sum(times) / len(times)
        size_score = max(0, 100 - (content_size / 1024 / 1024) * 10)  # MBë‹¹ 10ì  ê°ì 
        time_score = max(0, 100 - avg_time * 20)  # ì´ˆë‹¹ 20ì  ê°ì 
        return (size_score + time_score) / 2
    
    def _check_responsive_design(self, soup):
        viewport = soup.find('meta', attrs={'name': 'viewport'})
        media_queries = len(soup.find_all('style', string=re.compile(r'@media')))
        return bool(viewport) and media_queries > 0
    
    def _detect_technologies(self, headers, soup):
        technologies = []
        
        # ì„œë²„ ì •ë³´
        server = headers.get('server', '').lower()
        if 'nginx' in server:
            technologies.append('Nginx')
        elif 'apache' in server:
            technologies.append('Apache')
        
        # JavaScript í”„ë ˆì„ì›Œí¬
        scripts = soup.find_all('script', src=True)
        for script in scripts:
            src = script['src'].lower()
            if 'react' in src:
                technologies.append('React')
            elif 'vue' in src:
                technologies.append('Vue.js')
            elif 'angular' in src:
                technologies.append('Angular')
            elif 'jquery' in src:
                technologies.append('jQuery')
        
        return technologies
    
    def _check_mixed_content(self, html_content, is_https):
        if not is_https:
            return False
        return 'http://' in html_content and 'https://' in html_content
    
    def _calculate_mobile_score(self, viewport_content, media_queries):
        score = 0
        if viewport_content:
            score += 40
            if 'width=device-width' in viewport_content:
                score += 30
        if media_queries > 0:
            score += 30
        return min(score, 100)


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def main():
    analyzer = URLAnalyzer()
    
    # ë¶„ì„í•  URL
    test_url = "https://example.com"
    
    print("ğŸš€ URL ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    results = analyzer.analyze_url(test_url)
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    report = analyzer.generate_report(results, 'url_analysis_report.txt')
    print(report)
    
    # ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±
    dashboard_data = analyzer.create_dashboard_data(results)
    print("\nğŸ“Š ëŒ€ì‹œë³´ë“œ ë°ì´í„°:")
    print(json.dumps(dashboard_data, indent=2, ensure_ascii=False))
    
    # ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    with open('analysis_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    print("\nâœ… ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
